{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a763fd67",
   "metadata": {},
   "source": [
    "Extensiones usadas en el Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90028309",
   "metadata": {},
   "source": [
    "Extensiones usadas para el Desarrollo de los Algoritmos y Muestra de Resultados:\n",
    "\n",
    "1- numpy: procesamiento de los cálculos matriciales y vectoriales.\n",
    "\n",
    "2- matplotlib: visualización gráficas y tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc4dac",
   "metadata": {},
   "source": [
    "Funciones que calculan los valores de:\n",
    "\n",
    "1- Función Objetivo\n",
    "2- Vector Gradiente\n",
    "3- Matriz Hessiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b470b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función Objetivo\n",
    "def f(x):\n",
    "    return (np.exp(x[0]) + 1)*(x[1]**2 + 1) - np.sin(x[0] + x[1]**2) - x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5db11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Gradiente\n",
    "def grad_f(x):\n",
    "    df_dx = np.exp(x[0])*(x[1]**2 + 1) - np.cos(x[0] + x[1]**2) - 1\n",
    "    df_dy = 2*x[1]*(np.exp(x[0]) + 1) - 2*x[1]*np.cos(x[0] + x[1]**2)\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ae073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessiano \n",
    "def hess_f(x):\n",
    "    h11 = np.exp(x[0])*(x[1]**2 + 1) + np.sin(x[0] + x[1]**2)\n",
    "    h22 = 2*(np.exp(x[0]) + 1) - 2*np.cos(x[0] + x[1]**2) + 4*x[1]**2*np.sin(x[0] + x[1]**2)\n",
    "    h12 = 2*x[1]*np.exp(x[0]) + 2*x[1]*np.sin(x[0] + x[1]**2)\n",
    "    return np.array([[h11, h12], [h12, h22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d609996",
   "metadata": {},
   "source": [
    "Desarrollo Algoritmo Región de Confianza: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c85c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de Región de Confianza\n",
    "def trust_region_method(x0, grad_f, hess_f, delta0=1.0, eta=0.15, tol=1e-6, max_iter=1000):\n",
    "    \n",
    "    # x0: punto inicial\n",
    "    # grad_f: función que calcula el gradiente\n",
    "    # hess_f: función que calcula el Hessiano\n",
    "    # delta0: radio inicial de la región de confianza\n",
    "    # eta: umbral para aceptar el paso (0 < eta < 1)\n",
    "    # tol: tolerancia para criterio de parada\n",
    "    # max_iter: número máximo de iteraciones\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "    delta = delta0 #radio region de confianza\n",
    "\n",
    "    history = [x.copy()] #almacenar historial de puntos\n",
    "\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # 1- Calcular gradiente y Hessiana\n",
    "        g = grad_f(x)\n",
    "        B = hess_f(x)\n",
    "\n",
    "        if np.linalg.norm(g) < tol: # verificar gradiente es suficientemente pequeño\n",
    "            break\n",
    "        # 2- Resolver el subproblema Region COnfianza\n",
    "        try:\n",
    "            # Intentar calcular paso de Newton\n",
    "            p_newton = -np.linalg.solve(B, g)\n",
    "\n",
    "            # Verificar paso de Newton está dentro de la Region de COnfianza\n",
    "            if np.linalg.norm(p_newton) <= delta:\n",
    "                p = p_newton\n",
    "            else:\n",
    "                #calcular paso Cauchy => dirección máximo descenso\n",
    "                p_cauchy = - (np.dot(g, g) / (np.dot(g, np.dot(B, g)) + 1e-12)) * g \n",
    "\n",
    "                # Verificar si paso Cauchy dentro de la region de Confiaza\n",
    "                if np.linalg.norm(p_cauchy) > delta:\n",
    "                    p = -delta * g / np.linalg.norm(g)\n",
    "                else:\n",
    "                    p = delta * p_newton / np.linalg.norm(p_newton) #USAR paso Cauchy truncado al borde de la REgion\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Hessiano Singular => usar dirección de descenso más pronunciado\n",
    "            p = -delta * g / np.linalg.norm(g)\n",
    "\n",
    "        # 3- Evaluar calidad del Paso    \n",
    "        f_actual = f(x)\n",
    "        # valor real funcion en el nuevo paso\n",
    "        f_new = f(x + p)\n",
    "\n",
    "\n",
    "        # Aproximacion Taylor 2do Orden (Modelo Cuadrático)\n",
    "        m_new = f_actual + np.dot(g, p) + 0.5*np.dot(p, np.dot(B, p))\n",
    "        # Razon de Reducción: reducción real / reduccion predicha\n",
    "        rho = (f_actual - f_new) / (f_actual - m_new + 1e-12)\n",
    "        \n",
    "        # 4- Ajustar radio region de confianza\n",
    "        if rho < 0.25:\n",
    "            delta *= 0.25 # Reducir la region de cofianza\n",
    "        elif rho > 0.75:\n",
    "            delta = min(2*delta, 10) # expandir region de confianza\n",
    "        \n",
    "        # 5- Decición de aceptar o no el PASO\n",
    "        if rho > eta:\n",
    "            x = x + p # PASO ACEPTADO => moverse al nuevo punto\n",
    "            history.append(x.copy())\n",
    "        # PASO MUY GRANDE => PARAR    \n",
    "        if np.linalg.norm(p) < tol:\n",
    "            break\n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99e9b",
   "metadata": {},
   "source": [
    "Desarrollo Algoritmo Regulación Adaptativa Cúbica: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método ARC (Regularización Adaptativa Cúbica)\n",
    "def arc_method(x0, grad_f, hess_f, sigma0=1.0, tol=1e-6, max_iter=1000):\n",
    "    \n",
    "    # x0: punto inicial\n",
    "    # grad_f: función que calcula el gradiente\n",
    "    # hess_f: función que calcula el Hessiano  \n",
    "    # sigma0: parámetro de regularización inicial\n",
    "    # tol: tolerancia para criterio de parada\n",
    "    # max_iter: número máximo de iteraciones\n",
    "\n",
    "    x = np.array(x0, dtype=float)\n",
    "    sigma = sigma0 # Parametro Regulacion Cubica\n",
    "    history = [x.copy()] # Hiatorial puntos\n",
    "\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # 1- Calcular gradiente & Hessiano en el punto Actual\n",
    "        g = grad_f(x)\n",
    "        B = hess_f(x)\n",
    "\n",
    "        # Verificar si el gradiente es suficientemente pequeño\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "        # 2- Resolver Subproblema regularizador Cubico\n",
    "        try:\n",
    "            # Calcular paso => (B + sigma*I)p = -g\n",
    "            p = -np.linalg.solve(B + sigma*np.eye(len(x)), g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p = -g / (np.linalg.norm(g) + 1e-12) # Problemas Numericos => usar direccion descenso mas pronunciado\n",
    "        \n",
    "        # 3- Evaluar calidad del paso\n",
    "        f_actual = f(x)\n",
    "        f_new = f(x + p)  # valor real de la funcion en el nuevo punto \n",
    "        p_norm = np.linalg.norm(p)\n",
    "\n",
    "        # Taylor 2do orden + térmico cúbico (Modelo Cúbico)\n",
    "        m_new = f_actual + np.dot(g, p) + 0.5*np.dot(p, np.dot(B, p)) + (sigma/3)*p_norm**3\n",
    "\n",
    "        # Calcular razón de reducción => reducción real / reducción predicha\n",
    "        rho = (f_actual - f_new) / (f_actual - m_new + 1e-12)\n",
    "\n",
    "        # 4- Ajustar parametro regularizacion\n",
    "        if rho < 0.25:\n",
    "            sigma *= 2 # aumentar regularizacion => pasos más conservadores\n",
    "        elif rho > 0.75:\n",
    "            sigma = max(sigma/2, 1e-8) # disminuir regularizacion\n",
    "        \n",
    "        # 5- Decision acpetar o NO el paso-\n",
    "        if rho > 1e-4:\n",
    "            # Paso aceptado => moverse al nuevo punto\n",
    "            x = x + p\n",
    "            history.append(x.copy())\n",
    "\n",
    "        # paso muy pequeño => PARAR     \n",
    "        if np.linalg.norm(p) < tol:\n",
    "            break\n",
    "    return x, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0e47d",
   "metadata": {},
   "source": [
    "Ejecución y Ggraficación de los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCIÓN EN TRES RANGOS\n",
    "\n",
    "rangos = [(-2, 2), (-10, 10), (-100, 100)]\n",
    "resultados = []\n",
    "\n",
    "for rmin, rmax in rangos:\n",
    "    np.random.seed(42)\n",
    "    x0 = np.random.uniform(rmin, rmax, size=2)\n",
    "    x_tr, hist_tr = trust_region_method(x0, grad_f, hess_f)\n",
    "    x_arc, hist_arc = arc_method(x0, grad_f, hess_f)\n",
    "    resultados.append({\n",
    "        \"rango\": f\"[{rmin}, {rmax}]\",\n",
    "        \"x0\": x0,\n",
    "        \"x_tr\": x_tr,\n",
    "        \"x_arc\": x_arc,\n",
    "        \"hist_tr\": hist_tr,\n",
    "        \"hist_arc\": hist_arc,\n",
    "        \"f_tr\": np.array([f(x) for x in hist_tr]),\n",
    "        \"f_arc\": np.array([f(x) for x in hist_arc])\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# GRAFICAR [-2,2]\n",
    "\n",
    "res = resultados[0]\n",
    "x0, x_tr, x_arc = res[\"x0\"], res[\"x_tr\"], res[\"x_arc\"]\n",
    "hist_tr_arr = np.array(res[\"hist_tr\"])\n",
    "hist_arc_arr = np.array(res[\"hist_arc\"])\n",
    "f_tr, f_arc = res[\"f_tr\"], res[\"f_arc\"]\n",
    "\n",
    "# Crear malla vectorizada\n",
    "x_range = np.linspace(-2, 2, 200)\n",
    "y_range = np.linspace(-2, 2, 200)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = np.vectorize(lambda x, y: f([x, y]))(X, Y)\n",
    "\n",
    "# GRAFICOS 2D \n",
    "\n",
    "fig_graphs = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Vista 2D \n",
    "ax1 = fig_graphs.add_subplot(121)\n",
    "contour = ax1.contour(X, Y, Z, 30, cmap=cm.coolwarm_r, alpha=0.7)\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Trayectorias 2D\n",
    "ax1.plot(hist_tr_arr[:,0], hist_tr_arr[:,1], 'o-', color='red', linewidth=2, \n",
    "         markersize=6, markerfacecolor='darkred', markeredgecolor='black', \n",
    "         markeredgewidth=0.5, label='Región de Confianza')\n",
    "ax1.plot(hist_arc_arr[:,0], hist_arc_arr[:,1], 's-', color='blue', linewidth=2, \n",
    "         markersize=6, markerfacecolor='darkblue', markeredgecolor='black', \n",
    "         markeredgewidth=0.5, label='ARC')\n",
    "\n",
    "# Puntos importantes\n",
    "ax1.scatter([x0[0]], [x0[1]], color='green', s=150, marker='*', \n",
    "           edgecolor='black', linewidth=1, label='Punto Inicial', zorder=5)\n",
    "ax1.scatter([x_tr[0]], [x_tr[1]], color='red', s=150, marker='*', \n",
    "           edgecolor='black', linewidth=1, label='Óptimo TR', zorder=5)\n",
    "ax1.scatter([x_arc[0]], [x_arc[1]], color='blue', s=150, marker='*', \n",
    "           edgecolor='black', linewidth=1, label='Óptimo ARC', zorder=5)\n",
    "\n",
    "ax1.set_xlabel('x', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('y', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Trayectorias de Optimización [-2,2]', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.7)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Gráfico de convergencia\n",
    "ax2 = fig_graphs.add_subplot(122)\n",
    "f_min = min(f_tr[-1], f_arc[-1])\n",
    "\n",
    "ax2.semilogy(np.arange(len(f_tr)), f_tr - f_min, 'o-', color='red', linewidth=2.5,\n",
    "             markersize=6, markerfacecolor='darkred', markeredgecolor='black',\n",
    "             markeredgewidth=0.5, label='Región de Confianza')\n",
    "ax2.semilogy(np.arange(len(f_arc)), f_arc - f_min, 's-', color='blue', linewidth=2.5,\n",
    "             markersize=6, markerfacecolor='darkblue', markeredgecolor='black',\n",
    "             markeredgewidth=0.5, label='ARC')\n",
    "\n",
    "ax2.set_xlabel('Iteración', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('f(x) - f*', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Convergencia de la Función Objetivo [-2,2]', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TABLA COMPARATIVA (3 RANGOS)\n",
    "\n",
    "fig_table = plt.figure(figsize=(11, 4))\n",
    "ax_table = fig_table.add_subplot(111)\n",
    "ax_table.axis('tight')\n",
    "ax_table.axis('off')\n",
    "\n",
    "# Datos de la tabla\n",
    "table_data = [['Rango', 'Método', 'Iteraciones', 'f(x) final', '||∇f(x)|| final', 'Punto óptimo']]\n",
    "\n",
    "for res in resultados:\n",
    "    for metodo, hist, fx, xopt in [\n",
    "        ('Región de Confianza', res['hist_tr'], res['f_tr'], res['x_tr']),\n",
    "        ('ARC', res['hist_arc'], res['f_arc'], res['x_arc'])\n",
    "    ]:\n",
    "        table_data.append([\n",
    "            res['rango'],\n",
    "            metodo,\n",
    "            len(hist),\n",
    "            f\"{fx[-1]:.6e}\",\n",
    "            f\"{np.linalg.norm(grad_f(xopt)):.2e}\",\n",
    "            f\"({xopt[0]:.4f}, {xopt[1]:.4f})\"\n",
    "        ])\n",
    "\n",
    "# Crear tabla\n",
    "table = ax_table.table(cellText=table_data, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "for i, key in enumerate(table.get_celld().keys()):\n",
    "    cell = table.get_celld()[key]\n",
    "    if key[0] == 0:\n",
    "        cell.set_facecolor('#4B8BBE')\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax_table.set_title('COMPARACIÓN DE ALGORITMOS EN DIFERENTES RANGOS', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fbc17",
   "metadata": {},
   "source": [
    "Resultados Obtenidos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b486c",
   "metadata": {},
   "source": [
    "Gráficas de las Trayectorias y Convergencia de la Funion en el rango [-2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a591de",
   "metadata": {},
   "source": [
    "![Texto alternativo](TrayectoriaOpt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86edd48",
   "metadata": {},
   "source": [
    "![Texto alternativo](COnvergenciaFO.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee15a0",
   "metadata": {},
   "source": [
    "Tabla comparativa de Ambos Algoritmos donde se analiza:\n",
    "\n",
    "1- Número de Iteraciones\n",
    "2- Valor final de la funcion f(x,y)\n",
    "3- Valor final del vector Gradiente \n",
    "4- Punto Óptimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb0b7f",
   "metadata": {},
   "source": [
    "![Mi imagen](TablaDifRangos.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48b4cc",
   "metadata": {},
   "source": [
    "Resultados Obtenidos de los experimentos en los distintos rangos:\n",
    "\n",
    "Primer Rango [-2, 2]\n",
    "\n",
    "Segundo Rango [-10, 10]\n",
    "\n",
    "Tercer Rango [-100, 100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
